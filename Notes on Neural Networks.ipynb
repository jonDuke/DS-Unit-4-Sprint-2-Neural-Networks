{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Additional Resources\n",
    "\n",
    "- https://missinglink.ai/guides/neural-network-concepts/neural-network-bias-bias-neuron-overfitting-underfitting/\n",
    "- https://machinelearningmastery.com/neural-networks-are-function-approximators/\n",
    "- http://www.wildml.com/deep-learning-glossary/\n",
    "- https://machinelearningmastery.com/neural-networks-crash-course/\n",
    "- https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "- https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "- https://intellipaat.com/community/253/role-of-bias-in-neural-networks\n",
    "- https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron classification](https://images.slideplayer.com/32/9869800/slides/slide_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic formula for a simple perceptron\n",
    "\n",
    "### Single input formula\n",
    "`y = wx + bias`\n",
    "\n",
    "### Multi-input formula\n",
    "`y = (w1*x1 + w2*x2 + w3*x3 + wi*xi) + bias`\n",
    "\n",
    "- Where y is the output, w is the weight applied to the input, x is the input, and bias is a constant that helps activate the neuron.\n",
    "- This formula creates a straight line to linearly separate the classes\n",
    "- A single layer perceptron is only able to separate data points with a single line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Perceptron without an Activation Function\n",
    "\n",
    "- In the image below, $a_i$ indicates the inputs, $w_i$ indicates the weights for each input, $b$ is the bias added to the sum of the products, and $n$ is the number of inputs)\n",
    "- The image below does not show an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single layer perceptron equation](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTd676zk19cW48MOWxUjOSfuwiD-xFrx78T8zF8wgd6U5-UoZdJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Perceptron with an Activation Function\n",
    "\n",
    "- After the weights are multiplied to the inputs, the products are summed and passed to the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple Layer Perceptron w/out bias](https://www.allaboutcircuits.com/uploads/articles/how-to-train-a-basic-perceptron-neural-network_rk_aac_image1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "- The bias neuron is a special neuron added to each layer in the neural network, which simply stores the value of 1. This makes it possible to move or \"translate\" the activation function left or right on the graph.\n",
    "- Without a bias neuron, each neuron takes the input and multiplies it by a weight, with nothing else added to the equation.\n",
    "- So, for example, it is not possible to input a value of 0 and output 2.\n",
    "- In many cases, it is necessary to move the entire activation function to the left or right to generate the required output values -- this is made possible by the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image of Single Layer Perceptron with Bias\n",
    "\n",
    "- Constant is the bias\n",
    "- Step Function is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single layer perceptron](https://static.javatpoint.com/tutorial/pytorch/images/pytorch-perceptron2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and = {'X1': [0, 0, 1, 1], 'X2': [0, 1, 0, 1], 'output': [0, 0, 0, 1]}\n",
    "data_or  = {'X1': [0, 0, 1, 1], 'X2': [0, 1, 0, 1], 'output': [0, 1, 1, 1]}\n",
    "data_xor = {'X1': [0, 0, 1, 1], 'X2': [0, 1, 0, 1], 'output': [0, 1, 1, 0]}\n",
    "\n",
    "df_and = pd.DataFrame(data_and)\n",
    "df_or  = pd.DataFrame(data_or)\n",
    "df_xor = pd.DataFrame(data_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  output\n",
       "0   0   0       0\n",
       "1   0   1       0\n",
       "2   1   0       0\n",
       "3   1   1       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  output\n",
       "0   0   0       0\n",
       "1   0   1       1\n",
       "2   1   0       1\n",
       "3   1   1       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  output\n",
       "0   0   0       0\n",
       "1   0   1       1\n",
       "2   1   0       1\n",
       "3   1   1       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AND, OR, XOR Image](https://miro.medium.com/max/1920/1*CyGlr8VjwtQGeNsuTUq3HA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear separability](https://jtsulliv.github.io/images/perceptron/linsep_new.png?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND, OR, XOR Gates\n",
    "\n",
    "### AND\n",
    "\n",
    "- Output is True only if all inputs are true, otherwise False\n",
    "- The AND gate can be separated by a straight line since (1,0), (0,0), and (0,1) make up 1 class, and (1,1) make up the second class\n",
    "\n",
    "### OR\n",
    "\n",
    "- Output is True if any input is True, otherwise False if all inputs are False\n",
    "- The OR gate can be separated by a straight line since (1,0), (1,1), and (0,1) make up 1 class, and (0,0) make up the second class\n",
    "\n",
    "### XOR\n",
    "\n",
    "- Output is True only if 1 (and only 1) input is True, otherwise False if both inputs are True or both are False\n",
    "- XOR is not linearly separable because (0,0) and (1,1) make up one class, and (0,1) and (1,0) make up the second class (seen below)\n",
    "- There is no single straight line that is able to separate the two classes of the output, hence linearly non separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR Example](https://accu.org/content/images/journals/ol109/Lewin/Lewin-7b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the inputs cannot be separated by a straight line?\n",
    "\n",
    "- The XOR Problem\n",
    "- From the chart above, we can see that a straight line will not fit the data points.\n",
    "- A single perceptron is not sufficient to model XOR\n",
    "- The solution is to expand beyond the single-layer architecture by adding an additional layer of units known as a hidden layer (processing layer). This kind of architecture is called a multilayer perceptron (MLP).\n",
    "- An MLP is the basic building block of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![non linearly separable data](https://image.slidesharecdn.com/winnowvsperceptron-140225083320-phpapp02/95/winnow-vs-perceptron-7-638.jpg?cb=1393317299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron with 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multilayer Perceptron](https://www.researchgate.net/profile/Alex_Capatina/publication/313169214/figure/fig3/AS:669059322486797@1536527581843/Figure-no-5-The-architecture-of-neural-network-input-output-indicators.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron with 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP with 2 hidden layers](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTHr6qtOHOKrX4e085ZhM7WuHQUqbHbBf9fFZjlYWwmyWUjzjv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "- A neural network with many inputs, 3 or more hidden layers with several units (neurons), and an output layer at the end.\n",
    "- The number of output units (output neurons) depends on the classes we want to predict.\n",
    "- Example: If we are predicting if an image is a cat/dog/bird/cow, we would have 4 neurons at the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Deep Neural Network with 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fully connected deep neural network](https://miro.medium.com/max/432/1*y0pXhfaTGmvfNwaGoHnW5w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need an Activation Function `f(x)` to enable the model to learn complex data and represent non-linear relationships between the inputs and outputs.\n",
    "- The activation function is a mathematical \"gate\" in between the input going into the current neuron and its output going to the next layer.\n",
    "- Read more about activation functions here: https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![common activation functions](https://image.slidesharecdn.com/dlmmdcud1l02deepneuralnetworks-170427160932/95/deep-neural-networks-d1l2-insightdcu-machine-learning-workshop-2017-13-638.jpg?cb=1493309684)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation\n",
    "\n",
    "- Any input will be scaled to a value between 0-1\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- Gradient vanishes (zeroed out) when the input is small\n",
    "- Gradient explodes when the input is big\n",
    "- Optimization gets harder because the output isn't zero centered (it's centered at 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid Function](https://image1.slideserve.com/3099751/sigmoid-function-l.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tan Activation tanh(x)\n",
    "\n",
    "- The output is zero centered, so optimization is easier.\n",
    "- But still have the vanishing gradient problem\n",
    "- Common activation function for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hyperbolic Tan Activation](https://miro.medium.com/max/3450/1*aytYJ0uqNC1yhBzzAjns4A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "- `R(z) = max(0, z)` so ReLU outputs the max value between 0 and z (z being the input value)\n",
    "- If the input is negative --> Output is Zero\n",
    "- If the input is positive --> Output stays same value\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- If the gradients die, the ReLU never activates on any input. Therefore, we need to keep the gradients alive.\n",
    "- The problem occurs due to the Zero output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relu activation function](https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "- `R(x) = max(0.1*x, x)`\n",
    "- Solves the dying neuron problem, where gradients become zero by multiplying x by a near zero value (such as 0.1 or 0.01), then taking the max value of 0.1*x or x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![leaky relu activation function](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Activation Function should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *ReLU:* Cnns and hidden layers\n",
    "- *Leaky ReLU:* when ReLU suffers from dead neurons (output 0)\n",
    "- *Tanh:* RNNs\n",
    "- *Sigmoid:* Usually used for gating operations (controlling an output). Also used at the output layer for some tasks such as multi-label classification (classify more than one label at a given time) or binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent Map](https://blog.paperspace.com/content/images/2018/05/challenges-1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of learning rate](https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "- *Gradient Descent:* Can be defined as the change in y (where y is the amount of loss shown on y-axis above) due to the change in x (where x is the weights along the x-axis)\n",
    "- *Global Minimum (or Global minima):* The minimum point of the error function where modifying the weights no longer reduces loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backpropagation image](https://www.researchgate.net/profile/Rozaida_Ghazali/publication/234005707/figure/fig2/AS:667830315917314@1536234563135/The-structure-of-single-hidden-layer-MLP-with-Backpropagation-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backpropagation and gradient descent](https://miro.medium.com/max/1836/1*6sDUTAbKX_ICVVAjunCo3g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our objective is to minimize the error by changing the weights\n",
    "\n",
    "- We move in the direction opposite to the derivative (opposite to the slope)\n",
    "- *Negative Slope:* When we increase w, the loss function is decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
